

<!DOCTYPE html>


<html lang="fr" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Boosting &#8212; Apprentissage automatique</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/translations.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'adaboost';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Recherche" href="search.html" />
    <link rel="next" title="Méthodes de combinaison" href="combinaison.html" />
    <link rel="prev" title="Bootstraping et bagging" href="baggingboosting.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="fr"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Passer au contenu principal</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Apprentissage automatique - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Apprentissage automatique - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="possible.html">Exemple introductif</a></li>

<li class="toctree-l1"><a class="reference internal" href="approchestat.html">Modèle statistique de l’apprentissage</a></li>

<li class="toctree-l1"><a class="reference internal" href="modelesup.html">Modèle du processus d’apprentissage supervisé</a></li>


</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Classification</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="bayes.html">Classifieur naïf de Bayes</a></li>
<li class="toctree-l1"><a class="reference internal" href="LDAQDA.html">Analyses discriminantes</a></li>
<li class="toctree-l1"><a class="reference internal" href="knn.html">K plus proches voisins</a></li>
<li class="toctree-l1"><a class="reference internal" href="arbres_decision.html">Arbres de décision</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Méthodes à noyau</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="svmGeom.html">SVM linéaire</a></li>


<li class="toctree-l1"><a class="reference internal" href="kernelTrick.html">Astuce du noyau</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Méthodes d'ensemble</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="baggingboosting.html">Bootstraping et bagging</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Boosting</a></li>

<li class="toctree-l1"><a class="reference internal" href="combinaison.html">Méthodes de combinaison</a></li>
<li class="toctree-l1"><a class="reference internal" href="randomforest.html">Forêts aléatoires</a></li>
<li class="toctree-l1"><a class="reference internal" href="gradientboosting.html">Gradient Boosting</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Manifold learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="mds.html">Positionnement multidimensionnel</a></li>



<li class="toctree-l1"><a class="reference internal" href="isomap.html">ISOMAP</a></li>



<li class="toctree-l1"><a class="reference internal" href="lle.html">Local Linear Embedding</a></li>


<li class="toctree-l1"><a class="reference internal" href="manifold.html">Unification</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">TP</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="TPClassif.html">Comparaison de méthodes de classification supervisée</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Mode plein écran"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="clair/sombre" aria-label="clair/sombre" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Recherche" aria-label="Recherche" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Boosting</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contenu </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Boosting</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#adaboost">AdaBoost</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#algorithme">Algorithme</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-statistique">Interprétation statistique</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-comme-probleme-de-maximisation-d-une-marge">Interprétation comme problème de maximisation d’une marge</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="boosting">
<h1>Boosting<a class="headerlink" href="#boosting" title="Lien permanent vers cette rubrique">#</a></h1>
<p>Les méthodes d’ensemble sont des algorithmes d’apprentissage fondés sur l’idée qu’une combinaison de classifieurs simples (dits faibles), si elle est bien faite, doit donner de meilleurs résultats que chacun des classifieurs pris séparément. Le principe général suivant est adopté : il s’agit de construire une famille de modèles qui sont ensuite agrégés (moyenne pondérée des estimations,vote,…). Suivant la famille de modèles considérés (modèles dépendant les uns des autres, modèles indépendants), on aboutit à des stratégies différentes (boosting dans le premier cas, bagging, forêts aléatoires dans le second cas)</p>
<p>Dans ces méthodes, les notions de classifieurs faible est fort est fondamentale.
Considérons un problème de classification binaire, à valeurs dans <span class="math notranslate nohighlight">\(\{-1,1\}\)</span>. Soit un ensemble d’exemples</p>
<div class="math notranslate nohighlight">
\[Z=\left \{(\mathbf x_i,y_i),1\leq i\leq n, \mathbf x_i\in X,y_i\in \{-1,1\} \right \}\]</div>
<p>les <span class="math notranslate nohighlight">\(\mathbf x_i\)</span> étant des échantillons d’une certaine distribution <span class="math notranslate nohighlight">\(P\)</span>,  et <span class="math notranslate nohighlight">\(y_i=f(\mathbf x_i)\)</span>, <span class="math notranslate nohighlight">\(f\)</span> règle de classification. Un classifieur <span class="math notranslate nohighlight">\(h\)</span> est dit fort si, pour <span class="math notranslate nohighlight">\(n\)</span> suffisamment grand, il produit pour tout <span class="math notranslate nohighlight">\(P\)</span>, <span class="math notranslate nohighlight">\(f\)</span>, <span class="math notranslate nohighlight">\(\epsilon\geq 0\)</span> et <span class="math notranslate nohighlight">\(\delta\leq 1/2\)</span> une sortie avec probabilité plus petite que <span class="math notranslate nohighlight">\(1-\delta\)</span> telle que <span class="math notranslate nohighlight">\(\mathbb{P}_p(h(\mathbf x)\neq f(\mathbf x))\leq \epsilon\)</span>. De plus, la complexité temporelle de <span class="math notranslate nohighlight">\(h\)</span> doit au plus être polynomiale en <span class="math notranslate nohighlight">\(1/\epsilon\)</span>, <span class="math notranslate nohighlight">\(1/\delta\)</span> et <span class="math notranslate nohighlight">\(n\)</span>.\
A l’inverse, un classifieur faible produit, pour un certain <span class="math notranslate nohighlight">\(\epsilon_0\geq 0\)</span>, un certain <span class="math notranslate nohighlight">\(\delta_0\leq 1/2\)</span>, une sortie avec probabilité plus petite que <span class="math notranslate nohighlight">\(1-\delta_0\)</span>  telle que <span class="math notranslate nohighlight">\(\mathbb{P}_p(h(\mathbf x)\neq f(\mathbf x))\leq \epsilon_0\)</span>. En pratique, souvent, les classifieurs faibles produisent des résultats à peine meilleurs que l’aléatoire (dans le cas de la classification binaire, un tirage uniforme sur  <span class="math notranslate nohighlight">\(\{-1,1\}\)</span>).</p>
<p>Le boosting considère la construction d’une famille de modèles dépendant les uns des autres. Chaque modèle est une version adaptative du précédent en donnant plus de poids, lors de l’estimation suivante, aux observations mal ajustées ou mal prédites. Intuitivement, ces algorithmes concentrent donc leurs efforts sur les observations les plus difficiles à ajuster tandis que l’agrégation de l’ensemble des modèles permet d’échapper au surajustement.</p>
<p>Les algorithmes de boosting  diffèrent par plusieurs caractéristiques :</p>
<ul class="simple">
<li><p>la façon de pondérer c’est-à-dire de renforcer l’importance des observations mal estimées lors de l’itération précédente ;</p></li>
<li><p>leur objectif selon le type de la variable à prédire  : binaire, qualitative à <span class="math notranslate nohighlight">\(C\)</span> classes, réelles ;</p></li>
<li><p>la fonction perte, qui peut être choisie plus ou moins robuste aux valeurs atypiques, pour mesurer l’erreur d’ajustement ;</p></li>
<li><p>la façon de pondérer les classifieurs successifs.</p></li>
</ul>
<p>Le premier algorithme de boosting a été en 1990, dans lequel un classifieur fort est construit par combinaison de classifieurs faibles.</p>
<div class="proof algorithm admonition" id="stacking-algorithm">
<p class="admonition-title"><span class="caption-number">Algorithm 6 </span></p>
<section class="algorithm-content" id="proof-content">
<p><strong>Entrée</strong> : l’ensemble d’apprentissage <span class="math notranslate nohighlight">\(Z\)</span></p>
<p><strong>Sortie</strong> : Un classifieur <span class="math notranslate nohighlight">\(h\)</span></p>
<ol class="arabic simple">
<li><p>Z_1<span class="math notranslate nohighlight">\( : sous-ensemble de \)</span>n_1&lt;n<span class="math notranslate nohighlight">\( exemples de \)</span>Z$, tirés aléatoirement sans remise</p></li>
<li><p>apprentissage d’un classifieur faible <span class="math notranslate nohighlight">\(h_1\)</span> sur <span class="math notranslate nohighlight">\(Z_1\)</span></p></li>
<li><p>apprentissage d’un classifieur faible <span class="math notranslate nohighlight">\(h_2\)</span> sur <span class="math notranslate nohighlight">\(Z_2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(Z_3\)</span> : ensemble des exemples sur lesquels <span class="math notranslate nohighlight">\(h_1\)</span> et <span class="math notranslate nohighlight">\(h_2\)</span> sont en désaccord</p></li>
<li><p>apprentissage d’un classifieur faible <span class="math notranslate nohighlight">\(h_3\)</span> sur <span class="math notranslate nohighlight">\(Z_3\)</span></p></li>
</ol>
<p><span class="math notranslate nohighlight">\(h(\mathbf x) = sign\left (\displaystyle\sum_{i=1}^3 h_i(\mathbf x) \right)\)</span></p>
</section>
</div></section>
<section class="tex2jax_ignore mathjax_ignore" id="adaboost">
<h1>AdaBoost<a class="headerlink" href="#adaboost" title="Lien permanent vers cette rubrique">#</a></h1>
<p>AdaBoost (Adaptive Boosting) propose d’utiliser des versions pondérées du même ensemble d’apprentissage, plutôt que des sous-ensembles produits aléatoirement.</p>
<section id="algorithme">
<h2>Algorithme<a class="headerlink" href="#algorithme" title="Lien permanent vers cette rubrique">#</a></h2>
<p>Soit un problème de classification à deux classes. On dispose d’un ensemble d’apprentissage  <span class="math notranslate nohighlight">\(Z\)</span> et on cherche à évaluer la classe d’un point <span class="math notranslate nohighlight">\(\mathbf x\in X\)</span>.  On dispose de <span class="math notranslate nohighlight">\(M\)</span> classifieurs faibles <span class="math notranslate nohighlight">\(h_i\)</span> donnant une classification faible de <span class="math notranslate nohighlight">\(\mathbf x\)</span>. On souhaite construire un classifieur <span class="math notranslate nohighlight">\(h\)</span>, à valeurs dans <span class="math notranslate nohighlight">\(\{-1, 1\}\)</span>, combinaison linéaire des <span class="math notranslate nohighlight">\(h_i\)</span>.</p>
<p>Dans cet algorithme, le modèle de base retourne l’identité d’une classe, il est encore nommé AdaBoost discret (<a class="reference internal" href="#adaboost-algorithm">Algorithm 7</a>). Il est facile de l’adapter à des modèles retournant une valeur réelle comme une probabilité d’appartenance à une classe.</p>
<div class="proof algorithm admonition" id="adaboost-algorithm">
<p class="admonition-title"><span class="caption-number">Algorithm 7 </span></p>
<section class="algorithm-content" id="proof-content">
<p><strong>Entrée</strong> : l’ensemble d’apprentissage <span class="math notranslate nohighlight">\(Z\)</span>, \mathbf x<span class="math notranslate nohighlight">\(, \)</span>M$</p>
<p><strong>Sortie</strong> : Un classifieur <span class="math notranslate nohighlight">\(h\)</span></p>
<p>Initialisation des poids <span class="math notranslate nohighlight">\(\mathbf w\)</span> : <span class="math notranslate nohighlight">\(\forall i\in\{1\cdots n\},(w_i=\frac{1}{n})\)</span></p>
<p>Pour <span class="math notranslate nohighlight">\(i=1\)</span> à <span class="math notranslate nohighlight">\(M\)</span></p>
<ol class="arabic simple">
<li><p>Estimer <span class="math notranslate nohighlight">\(h_i\)</span> sur <span class="math notranslate nohighlight">\(Z\)</span> pondéré par <span class="math notranslate nohighlight">\(w\)</span></p></li>
<li><p>Calculer le taux d’erreur</p></li>
<li><p>\epsilon_i = \displaystyle\sum_{j=1}^n\mathbf{1}_{h_i(\mathbf x_j)\neq y_j} $</p></li>
<li><p>Calculer le poids du classifieur faible <span class="math notranslate nohighlight">\(\alpha_i\leftarrow \frac{1}{2}log\left ( \frac{1-\epsilon_i}{\epsilon_i}\right)\)</span></p></li>
<li><p>Pour <span class="math notranslate nohighlight">\(j=1\)</span> à <span class="math notranslate nohighlight">\(n\)</span> : <span class="math notranslate nohighlight">\(w_j\leftarrow w_j exp\left[ -\alpha_iy_jh_i(\mathbf x_j)\right]\)</span></p></li>
<li><p>Renormaliser les poids : <span class="math notranslate nohighlight">\(W= \displaystyle\sum_{j=1}^n w_j\)</span>\</p></li>
<li><p>Pour <span class="math notranslate nohighlight">\(j=1\)</span> à <span class="math notranslate nohighlight">\(n\)</span> : <span class="math notranslate nohighlight">\(w_j\leftarrow w_j/W\)</span></p></li>
</ol>
<p><span class="math notranslate nohighlight">\(h(\mathbf x)=sign\left [\displaystyle\sum_{j=1}^M\alpha_jh_j(\mathbf x) \right]\)</span></p>
</section>
</div><p>Les poids de chaque observation sont initialisés à <span class="math notranslate nohighlight">\({1}{/n}\)</span> pour l’estimation du premier modèle, puis évoluent à chaque itération donc pour chaque nouvelle estimation. Le poids d’un exemple est inchangé si ce dernier est bien classé, il croît sinon proportionnellement au défaut d’ajustement du modèle. Estimer <span class="math notranslate nohighlight">\(h_i\)</span> sur <span class="math notranslate nohighlight">\(Z\)</span> pondéré par <span class="math notranslate nohighlight">\(\mathbf w\)</span> signifie trouver le classifieur faible parmi une famille de classifieurs satisfaisant</p>
<div class="math notranslate nohighlight">
\[\displaystyle\sum_{j=1}^n w_j\mathbf{1}_{h_i(\mathbf x_j)\neq y_j}\leq \frac{1}{2}-\epsilon\]</div>
<p>pour un petit <span class="math notranslate nohighlight">\(\epsilon\)</span>.</p>
<p>L’agrégation finale des prévisions :</p>
<div class="math notranslate nohighlight">
\[\displaystyle\sum_{i=1}^M\alpha_ih_i(\mathbf x)\]</div>
<p>est une combinaison pondérée par les qualités d’ajustement de chaque modèle. Sa valeur absolue appelée marge est proportionnelle à la confiance que l’on peut attribuer à son signe qui fournit le résultat de la prévision.</p>
<p>Après <span class="math notranslate nohighlight">\(M\)</span> itérations, les exemples avec un très fort poids sont des exemples durs à apprendre, et possiblement des points aberrants. AdaBoost peut donc servir à détecter des outliers sur un ensemble d’apprentissage <span class="math notranslate nohighlight">\(Z\)</span> donné.</p>
<p>Notons qu’il est possible d’étendre AdaBoost à des problèmes de régression.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">AdaBoostClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_moons</span>



<span class="k">def</span> <span class="nf">plot_decision_boundary</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">contour</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">x1s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">x2s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">axes</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x1s</span><span class="p">,</span> <span class="n">x2s</span><span class="p">)</span>
    <span class="n">X_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">x1</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">x2</span><span class="o">.</span><span class="n">ravel</span><span class="p">()]</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">mapp</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s1">&#39;#FF0000&#39;</span><span class="p">,</span> <span class="s1">&#39;#00FF00&#39;</span><span class="p">])</span>
    <span class="nb">map</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s1">&#39;#fafab0&#39;</span><span class="p">,</span><span class="s1">&#39;#9898ff&#39;</span><span class="p">,</span><span class="s1">&#39;#a0faa0&#39;</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="nb">map</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">contour</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">mapp</span><span class="p">,</span><span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x_1$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x_2$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_moons</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">700</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.30</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">nb_weak</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1"># Adaboost sur arbres de décision</span>
<span class="n">adaDT_clf</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> 
                               <span class="n">n_estimators</span><span class="o">=</span><span class="n">nb_weak</span><span class="p">,</span><span class="n">algorithm</span><span class="o">=</span><span class="s2">&quot;SAMME.R&quot;</span><span class="p">,</span> 
                               <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="c1"># Adaboost sur SVM</span>
<span class="n">adaSVC_clf</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s2">&quot;rbf&quot;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span><span class="n">gamma</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">),</span> 
                                <span class="n">n_estimators</span><span class="o">=</span><span class="n">nb_weak</span><span class="p">,</span><span class="n">algorithm</span><span class="o">=</span><span class="s2">&quot;SAMME&quot;</span><span class="p">,</span> 
                                <span class="n">learning_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="c1"># Classifieur de Bayes naif gaussien</span>
<span class="n">adaGAU_clf</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span><span class="n">GaussianNB</span><span class="p">(),</span> <span class="n">n_estimators</span><span class="o">=</span><span class="n">nb_weak</span><span class="p">,</span>
    <span class="n">algorithm</span><span class="o">=</span><span class="s2">&quot;SAMME.R&quot;</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="k">for</span> <span class="n">clf</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">title</span> <span class="ow">in</span> <span class="p">((</span><span class="n">adaDT_clf</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="s1">&#39;Arbres de décision&#39;</span><span class="p">),</span> 
                    <span class="p">(</span><span class="n">adaSVC_clf</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="s1">&#39;SVM&#39;</span><span class="p">),</span> 
                    <span class="p">(</span><span class="n">adaGAU_clf</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="s1">&#39;Classifieur naïf gaussien&#39;</span><span class="p">)):</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">i</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">plot_decision_boundary</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/805836746b063fe5c2a23e5592411634bf1bc5a121cb999760cae224f5a57d19.png" src="_images/805836746b063fe5c2a23e5592411634bf1bc5a121cb999760cae224f5a57d19.png" />
</div>
</div>
</section>
<section id="interpretation-statistique">
<h2>Interprétation statistique<a class="headerlink" href="#interpretation-statistique" title="Lien permanent vers cette rubrique">#</a></h2>
<p>Il est possible d’interpréter Adaboost en termes statistiques, pour justifier en partie le bon comportement du boosting en classification. Pour ce faire, on définit les classifieurs faibles comme des fonctions paramétriques <span class="math notranslate nohighlight">\(h_{\theta_i}(\mathbf x) = h_i(\mathbf x,\theta)\)</span>, <span class="math notranslate nohighlight">\(\theta\in\Theta\)</span>, et le résultat de la classification comme une combinaison linéaire de ces classifieurs faibles <span class="math notranslate nohighlight">\(h(\mathbf x)=\displaystyle\sum_{i=1}^n \alpha_i h_{\theta_i}(\mathbf x)\)</span>.</p>
<p>Une approche pour déterminer les <span class="math notranslate nohighlight">\(\theta_i\)</span> et les <span class="math notranslate nohighlight">\(\alpha_i\)</span> est d’ajouter séquentiellement au problème d’apprentissage les classifieurs faibles, sans ajuster les paramètres et les coefficients de la solution courante (<a class="reference internal" href="#adaboost-stat-algorithm">Algorithm 8</a>).</p>
<div class="proof algorithm admonition" id="adaboost-stat-algorithm">
<p class="admonition-title"><span class="caption-number">Algorithm 8 </span></p>
<section class="algorithm-content" id="proof-content">
<p><strong>Entrée</strong> : l’ensemble d’apprentissage <span class="math notranslate nohighlight">\(Z\)</span>, \mathbf x<span class="math notranslate nohighlight">\(, \)</span>M$</p>
<p><strong>Sortie</strong> : Un classifieur <span class="math notranslate nohighlight">\(h\)</span></p>
<p>Initialisation des poids <span class="math notranslate nohighlight">\(\mathbf w\)</span> : <span class="math notranslate nohighlight">\(\forall i\in\{1\cdots n\},(w_i=\frac{1}{n})\)</span></p>
<p>Pour <span class="math notranslate nohighlight">\(i=1\)</span> à <span class="math notranslate nohighlight">\(M\)</span></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(h_0(\mathbf x) = 0\)</span></p></li>
</ol>
<p>Pour <span class="math notranslate nohighlight">\(i=1\)</span> à <span class="math notranslate nohighlight">\(M\)</span></p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>1. $(\alpha_i,\theta_i) = arg\displaystyle\min_{\alpha\in\mathbb{R}^+,\theta\in\Theta} \displaystyle\sum_{j=1}^n L(y_jh_{i-1}(\mathbf x_j)+\alpha h_{\theta_i}(\mathbf x_j))$
2. $h_i(\mathbf x) = h_{i-1}(\mathbf x)+\alpha_i h_{\theta_i}(\mathbf x)$
</pre></div>
</div>
<p><span class="math notranslate nohighlight">\(h(\mathbf x)=\displaystyle\sum_{i=1}^n \alpha_i h_{\theta_i}(\mathbf x)\)</span></p>
</section>
</div><p>Cet algorithme, lorsque l’on utilise la fonction de perte exponentielle <span class="math notranslate nohighlight">\(L(y,f(\mathbf x)) = e^{-yf(\mathbf x)}\)</span>, est équivalent à l’algorithme Adaboost. En effet, à chaque itération, la minimisation suivante est effectuée :</p>
<div class="math notranslate nohighlight">
\[\begin{split} 
(\alpha_i,\theta_i) &amp;=&amp; arg\min_{\alpha\in\mathbb{R}^+,\theta\in\Theta} \displaystyle\sum_{j=1}^nexp\left [-y_j(h_{i-1}(\mathbf x_j)+\alpha h_{\theta_i}(\mathbf x_j)) \right ]\\
&amp;=&amp; arg\min_{\alpha\in\mathbb{R}^+,\theta\in\Theta} \displaystyle\sum_{j=1}^nexp\left [-y_jh_{i-1}(\mathbf x_j)\right ]exp\left [-y_j\alpha h_{\theta_i}(\mathbf x_j) \right ]\\
&amp;=&amp; arg\min_{\alpha\in\mathbb{R}^+,\theta\in\Theta} \displaystyle\sum_{j=1}^n w_j^i exp\left[-y_j\alpha h_{\theta_i}(\mathbf x_j) \right]
\label{eqWeakMin}
\end{split}\]</div>
<p>où <span class="math notranslate nohighlight">\(w_j^i = exp\left [-y_j h_{i-1}(\mathbf x_j) \right]\)</span> n’affecte pas le problème d’optimisation. Pour tout <span class="math notranslate nohighlight">\(\alpha&gt;0\)</span>, la fonction objectif peut être réécrite</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\theta_i &amp;=&amp; arg\min_{\theta\in\Theta} \left [e^{-\alpha}\displaystyle\sum_{y_j=h_{\theta_i}(\mathbf x_j)} w_j^t +e^\alpha \displaystyle\sum_{y_j\neq h_{\theta_i}(\mathbf x_j)} w_j^i \right ]\\
&amp;=&amp; arg\min_{\theta\in\Theta} \left [(e^{-\alpha}+e^\alpha)\displaystyle\sum_{j=1}^nw_j^i \mathbb{I}_{\{y_j\neq h_{\theta_i}(\mathbf x_j)\}} +e^\alpha \displaystyle\sum_{j=1}^n w_j^i \right ]\\
&amp;=&amp;arg\min_{\theta\in\Theta} \displaystyle\sum_{j=1}^nw_j^i \mathbb{I}_{\{y_j\neq h_{\theta_i}(\mathbf x_j)\}} \\
\end{split}\]</div>
<p>Le classifieur faible minimisant (\ref{eqWeakMin}) minimisera donc également le taux d’erreur pondéré, que l’on réinjecte dans (\ref{eqWeakMin}) pour trouver</p>
<div class="math notranslate nohighlight">
\[\alpha_i=\frac{1}{2}log\frac{1-\epsilon_i}{\epsilon_i}\]</div>
<p>avec</p>
<div class="math notranslate nohighlight">
\[\epsilon_i= \displaystyle\sum_{j=1}^nw_j^i \mathbb{I}_{\{y_j\neq h_{\theta_i}(\mathbf x_j)\}}\]</div>
<p>Enfin, la mise à jour du modèle <span class="math notranslate nohighlight">\(h_i(\mathbf x) = h_{i-1}(\mathbf x)+\alpha_ih_{\theta_i}(\mathbf x)\)</span> est équivalente à la mise à jour des poids dans AdaBoost, puisque</p>
<div class="math notranslate nohighlight">
\[w_j^{i+1}=w_j^{i}e^{-y_jh_i(\mathbf x_j)}\]</div>
<p>En résumé, AdaBoost peut être interprété comme un algorithme minimisant la fonction de perte exponentielle par ajout itératif de classifieurs faibles.</p>
</section>
<section id="interpretation-comme-probleme-de-maximisation-d-une-marge">
<h2>Interprétation comme problème de maximisation d’une marge<a class="headerlink" href="#interpretation-comme-probleme-de-maximisation-d-une-marge" title="Lien permanent vers cette rubrique">#</a></h2>
<p>Il est également possible de relier AdaBoost à un problème de séparation à vaste marge.</p>
<p>Le principe est de construire un espace de coordonnées de dimension égale au nombre de classifieurs faibles, <span class="math notranslate nohighlight">\(M\)</span>, et pour <span class="math notranslate nohighlight">\(\mathbf u\in \mathbb{R}^M\)</span> de considérer les coordonnées <span class="math notranslate nohighlight">\(u_i\)</span> comme les sorties des classifieurs faibles <span class="math notranslate nohighlight">\(h_i\)</span>. On montre alors qu’AdaBoost est un algorithme itératif qui résout le problème d’optimisation suivant</p>
<div class="math notranslate nohighlight">
\[\hat{w} = arg \max_{\mathbf w\in\mathbb{R}^M}\min_{\mathbf{u_j}\in\mathbb{R}^M}\frac{y_j\mathbf w^T\mathbf{u_j}}{\|\mathbf w\|_{L_1}}\]</div>
<p>où <span class="math notranslate nohighlight">\(\mathbf{u_j}=(h_1(\mathbf x_j)\cdots h_M(\mathbf x_j))\)</span> et où le classifieur final est</p>
<div class="math notranslate nohighlight">
\[h(\mathbf x) = \displaystyle\sum_{i=1}^M\hat{w}_ih_i(\mathbf x)\]</div>
<p>ce qui correspond à un problème de maximisation de la plus petite des distance entre des points de classes différentes, soit un problème de maximisation de marge.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="baggingboosting.html"
       title="page précédente">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">précédent</p>
        <p class="prev-next-title">Bootstraping et bagging</p>
      </div>
    </a>
    <a class="right-next"
       href="combinaison.html"
       title="page suivante">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">suivant</p>
        <p class="prev-next-title">Méthodes de combinaison</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contenu
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Boosting</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#adaboost">AdaBoost</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#algorithme">Algorithme</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-statistique">Interprétation statistique</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-comme-probleme-de-maximisation-d-une-marge">Interprétation comme problème de maximisation d’une marge</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
Par Vincent BARRA
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>