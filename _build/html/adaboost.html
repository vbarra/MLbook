
<!DOCTYPE html>

<html lang="fr">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Boosting &#8212; Apprentissage automatique</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/translations.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Recherche" href="search.html" />
    <link rel="next" title="Méthodes de combinaison" href="combinaison.html" />
    <link rel="prev" title="Bootstraping et bagging" href="baggingboosting.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="fr">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Apprentissage automatique</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="possible.html">
   Exemple introductif
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="approchestat.html">
   Modèle statistique de l’apprentissage
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="modelesup.html">
   Modèle du processus d’apprentissage supervisé
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Classification
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="bayes.html">
   Classifieur naïf de Bayes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="LDAQDA.html">
   Analyses discriminantes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="knn.html">
   K plus proches voisins
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="arbres_decision.html">
   Arbres de décision
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Méthodes à noyau
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="svmGeom.html">
   SVM linéaire
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kernelTrick.html">
   Astuce du noyau
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Méthodes d'ensemble
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="baggingboosting.html">
   Bootstraping et bagging
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Boosting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="combinaison.html">
   Méthodes de combinaison
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="randomforest.html">
   Forêts aléatoires
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gradientboosting.html">
   Gradient Boosting
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Manifold learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="mds.html">
   Positionnement multidimensionnel
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="isomap.html">
   ISOMAP
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lle.html">
   Local Linear Embedding
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="manifold.html">
   Unification
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  TP
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="TPClassif.html">
   Comparaison de méthodes de classification supervisée
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Boosting
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#adaboost">
   AdaBoost
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#algorithme">
     Algorithme
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpretation-statistique">
     Interprétation statistique
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpretation-comme-probleme-de-maximisation-d-une-marge">
     Interprétation comme problème de maximisation d’une marge
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Boosting</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Boosting
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#adaboost">
   AdaBoost
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#algorithme">
     Algorithme
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpretation-statistique">
     Interprétation statistique
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpretation-comme-probleme-de-maximisation-d-une-marge">
     Interprétation comme problème de maximisation d’une marge
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="boosting">
<h1>Boosting<a class="headerlink" href="#boosting" title="Lien permanent vers ce titre">#</a></h1>
<p>Les méthodes d’ensemble sont des algorithmes d’apprentissage fondés sur l’idée qu’une combinaison de classifieurs simples (dits faibles), si elle est bien faite, doit donner de meilleurs résultats que chacun des classifieurs pris séparément. Le principe général suivant est adopté : il s’agit de construire une famille de modèles qui sont ensuite agrégés (moyenne pondérée des estimations,vote,…). Suivant la famille de modèles considérés (modèles dépendant les uns des autres, modèles indépendants), on aboutit à des stratégies différentes (boosting dans le premier cas, bagging, forêts aléatoires dans le second cas)</p>
<p>Dans ces méthodes, les notions de classifieurs faible est fort est fondamentale.
Considérons un problème de classification binaire, à valeurs dans <span class="math notranslate nohighlight">\(\{-1,1\}\)</span>. Soit un ensemble d’exemples</p>
<div class="math notranslate nohighlight">
\[Z=\left \{(\mathbf x_i,y_i),1\leq i\leq n, \mathbf x_i\in X,y_i\in \{-1,1\} \right \}\]</div>
<p>les <span class="math notranslate nohighlight">\(\mathbf x_i\)</span> étant des échantillons d’une certaine distribution <span class="math notranslate nohighlight">\(P\)</span>,  et <span class="math notranslate nohighlight">\(y_i=f(\mathbf x_i)\)</span>, <span class="math notranslate nohighlight">\(f\)</span> règle de classification. Un classifieur <span class="math notranslate nohighlight">\(h\)</span> est dit fort si, pour <span class="math notranslate nohighlight">\(n\)</span> suffisamment grand, il produit pour tout <span class="math notranslate nohighlight">\(P\)</span>, <span class="math notranslate nohighlight">\(f\)</span>, <span class="math notranslate nohighlight">\(\epsilon\geq 0\)</span> et <span class="math notranslate nohighlight">\(\delta\leq 1/2\)</span> une sortie avec probabilité plus petite que <span class="math notranslate nohighlight">\(1-\delta\)</span> telle que <span class="math notranslate nohighlight">\(\mathbb{P}_p(h(\mathbf x)\neq f(\mathbf x))\leq \epsilon\)</span>. De plus, la complexité temporelle de <span class="math notranslate nohighlight">\(h\)</span> doit au plus être polynomiale en <span class="math notranslate nohighlight">\(1/\epsilon\)</span>, <span class="math notranslate nohighlight">\(1/\delta\)</span> et <span class="math notranslate nohighlight">\(n\)</span>.\
A l’inverse, un classifieur faible produit, pour un certain <span class="math notranslate nohighlight">\(\epsilon_0\geq 0\)</span>, un certain <span class="math notranslate nohighlight">\(\delta_0\leq 1/2\)</span>, une sortie avec probabilité plus petite que <span class="math notranslate nohighlight">\(1-\delta_0\)</span>  telle que <span class="math notranslate nohighlight">\(\mathbb{P}_p(h(\mathbf x)\neq f(\mathbf x))\leq \epsilon_0\)</span>. En pratique, souvent, les classifieurs faibles produisent des résultats à peine meilleurs que l’aléatoire (dans le cas de la classification binaire, un tirage uniforme sur  <span class="math notranslate nohighlight">\(\{-1,1\}\)</span>).</p>
<p>Le boosting considère la construction d’une famille de modèles dépendant les uns des autres. Chaque modèle est une version adaptative du précédent en donnant plus de poids, lors de l’estimation suivante, aux observations mal ajustées ou mal prédites. Intuitivement, ces algorithmes concentrent donc leurs efforts sur les observations les plus difficiles à ajuster tandis que l’agrégation de l’ensemble des modèles permet d’échapper au surajustement.</p>
<p>Les algorithmes de boosting  diffèrent par plusieurs caractéristiques :</p>
<ul class="simple">
<li><p>la façon de pondérer c’est-à-dire de renforcer l’importance des observations mal estimées lors de l’itération précédente ;</p></li>
<li><p>leur objectif selon le type de la variable à prédire  : binaire, qualitative à <span class="math notranslate nohighlight">\(C\)</span> classes, réelles ;</p></li>
<li><p>la fonction perte, qui peut être choisie plus ou moins robuste aux valeurs atypiques, pour mesurer l’erreur d’ajustement ;</p></li>
<li><p>la façon de pondérer les classifieurs successifs.</p></li>
</ul>
<p>Le premier algorithme de boosting a été en 1990, dans lequel un classifieur fort est construit par combinaison de classifieurs faibles.</p>
<div class="proof algorithm admonition" id="stacking-algorithm">
<p class="admonition-title"><span class="caption-number">Algorithm 6 </span></p>
<div class="algorithm-content section" id="proof-content">
<p><strong>Entrée</strong> : l’ensemble d’apprentissage <span class="math notranslate nohighlight">\(Z\)</span></p>
<p><strong>Sortie</strong> : Un classifieur <span class="math notranslate nohighlight">\(h\)</span></p>
<ol class="simple">
<li><p>Z_1<span class="math notranslate nohighlight">\( : sous-ensemble de \)</span>n_1&lt;n<span class="math notranslate nohighlight">\( exemples de \)</span>Z$, tirés aléatoirement sans remise</p></li>
<li><p>apprentissage d’un classifieur faible <span class="math notranslate nohighlight">\(h_1\)</span> sur <span class="math notranslate nohighlight">\(Z_1\)</span></p></li>
<li><p>apprentissage d’un classifieur faible <span class="math notranslate nohighlight">\(h_2\)</span> sur <span class="math notranslate nohighlight">\(Z_2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(Z_3\)</span> : ensemble des exemples sur lesquels <span class="math notranslate nohighlight">\(h_1\)</span> et <span class="math notranslate nohighlight">\(h_2\)</span> sont en désaccord</p></li>
<li><p>apprentissage d’un classifieur faible <span class="math notranslate nohighlight">\(h_3\)</span> sur <span class="math notranslate nohighlight">\(Z_3\)</span></p></li>
</ol>
<p><span class="math notranslate nohighlight">\(h(\mathbf x) = sign\left (\displaystyle\sum_{i=1}^3 h_i(\mathbf x) \right)\)</span></p>
</div>
</div></div>
<div class="tex2jax_ignore mathjax_ignore section" id="adaboost">
<h1>AdaBoost<a class="headerlink" href="#adaboost" title="Lien permanent vers ce titre">#</a></h1>
<p>AdaBoost (Adaptive Boosting) propose d’utiliser des versions pondérées du même ensemble d’apprentissage, plutôt que des sous-ensembles produits aléatoirement.</p>
<div class="section" id="algorithme">
<h2>Algorithme<a class="headerlink" href="#algorithme" title="Lien permanent vers ce titre">#</a></h2>
<p>Soit un problème de classification à deux classes. On dispose d’un ensemble d’apprentissage  <span class="math notranslate nohighlight">\(Z\)</span> et on cherche à évaluer la classe d’un point <span class="math notranslate nohighlight">\(\mathbf x\in X\)</span>.  On dispose de <span class="math notranslate nohighlight">\(M\)</span> classifieurs faibles <span class="math notranslate nohighlight">\(h_i\)</span> donnant une classification faible de <span class="math notranslate nohighlight">\(\mathbf x\)</span>. On souhaite construire un classifieur <span class="math notranslate nohighlight">\(h\)</span>, à valeurs dans <span class="math notranslate nohighlight">\(\{-1, 1\}\)</span>, combinaison linéaire des <span class="math notranslate nohighlight">\(h_i\)</span>.</p>
<p>Dans cet algorithme, le modèle de base retourne l’identité d’une classe, il est encore nommé AdaBoost discret (<a class="reference internal" href="#adaboost-algorithm">Algorithm 7</a>). Il est facile de l’adapter à des modèles retournant une valeur réelle comme une probabilité d’appartenance à une classe.</p>
<div class="proof algorithm admonition" id="adaboost-algorithm">
<p class="admonition-title"><span class="caption-number">Algorithm 7 </span></p>
<div class="algorithm-content section" id="proof-content">
<p><strong>Entrée</strong> : l’ensemble d’apprentissage <span class="math notranslate nohighlight">\(Z\)</span>, \mathbf x<span class="math notranslate nohighlight">\(, \)</span>M$</p>
<p><strong>Sortie</strong> : Un classifieur <span class="math notranslate nohighlight">\(h\)</span></p>
<p>Initialisation des poids <span class="math notranslate nohighlight">\(\mathbf w\)</span> : <span class="math notranslate nohighlight">\(\forall i\in\{1\cdots n\},(w_i=\frac{1}{n})\)</span></p>
<p>Pour <span class="math notranslate nohighlight">\(i=1\)</span> à <span class="math notranslate nohighlight">\(M\)</span></p>
<ol class="simple">
<li><p>Estimer <span class="math notranslate nohighlight">\(h_i\)</span> sur <span class="math notranslate nohighlight">\(Z\)</span> pondéré par <span class="math notranslate nohighlight">\(w\)</span></p></li>
<li><p>Calculer le taux d’erreur</p></li>
<li><p>\epsilon_i = \displaystyle\sum_{j=1}^n\mathbf{1}_{h_i(\mathbf x_j)\neq y_j} $</p></li>
<li><p>Calculer le poids du classifieur faible <span class="math notranslate nohighlight">\(\alpha_i\leftarrow \frac{1}{2}log\left ( \frac{1-\epsilon_i}{\epsilon_i}\right)\)</span></p></li>
<li><p>Pour <span class="math notranslate nohighlight">\(j=1\)</span> à <span class="math notranslate nohighlight">\(n\)</span> : <span class="math notranslate nohighlight">\(w_j\leftarrow w_j exp\left[ -\alpha_iy_jh_i(\mathbf x_j)\right]\)</span></p></li>
<li><p>Renormaliser les poids : <span class="math notranslate nohighlight">\(W= \displaystyle\sum_{j=1}^n w_j\)</span>\</p></li>
<li><p>Pour <span class="math notranslate nohighlight">\(j=1\)</span> à <span class="math notranslate nohighlight">\(n\)</span> : <span class="math notranslate nohighlight">\(w_j\leftarrow w_j/W\)</span></p></li>
</ol>
<p><span class="math notranslate nohighlight">\(h(\mathbf x)=sign\left [\displaystyle\sum_{j=1}^M\alpha_jh_j(\mathbf x) \right]\)</span></p>
</div>
</div><p>Les poids de chaque observation sont initialisés à <span class="math notranslate nohighlight">\({1}{/n}\)</span> pour l’estimation du premier modèle, puis évoluent à chaque itération donc pour chaque nouvelle estimation. Le poids d’un exemple est inchangé si ce dernier est bien classé, il croît sinon proportionnellement au défaut d’ajustement du modèle. Estimer <span class="math notranslate nohighlight">\(h_i\)</span> sur <span class="math notranslate nohighlight">\(Z\)</span> pondéré par <span class="math notranslate nohighlight">\(\mathbf w\)</span> signifie trouver le classifieur faible parmi une famille de classifieurs satisfaisant</p>
<div class="math notranslate nohighlight">
\[\displaystyle\sum_{j=1}^n w_j\mathbf{1}_{h_i(\mathbf x_j)\neq y_j}\leq \frac{1}{2}-\epsilon\]</div>
<p>pour un petit <span class="math notranslate nohighlight">\(\epsilon\)</span>.</p>
<p>L’agrégation finale des prévisions :</p>
<div class="math notranslate nohighlight">
\[\displaystyle\sum_{i=1}^M\alpha_ih_i(\mathbf x)\]</div>
<p>est une combinaison pondérée par les qualités d’ajustement de chaque modèle. Sa valeur absolue appelée marge est proportionnelle à la confiance que l’on peut attribuer à son signe qui fournit le résultat de la prévision.</p>
<p>Après <span class="math notranslate nohighlight">\(M\)</span> itérations, les exemples avec un très fort poids sont des exemples durs à apprendre, et possiblement des points aberrants. AdaBoost peut donc servir à détecter des outliers sur un ensemble d’apprentissage <span class="math notranslate nohighlight">\(Z\)</span> donné.</p>
<p>Notons qu’il est possible d’étendre AdaBoost à des problèmes de régression.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">AdaBoostClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_moons</span>



<span class="k">def</span> <span class="nf">plot_decision_boundary</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">contour</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">x1s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">x2s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">axes</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x1s</span><span class="p">,</span> <span class="n">x2s</span><span class="p">)</span>
    <span class="n">X_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">x1</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">x2</span><span class="o">.</span><span class="n">ravel</span><span class="p">()]</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">mapp</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s1">&#39;#FF0000&#39;</span><span class="p">,</span> <span class="s1">&#39;#00FF00&#39;</span><span class="p">])</span>
    <span class="nb">map</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s1">&#39;#fafab0&#39;</span><span class="p">,</span><span class="s1">&#39;#9898ff&#39;</span><span class="p">,</span><span class="s1">&#39;#a0faa0&#39;</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="nb">map</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">contour</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">mapp</span><span class="p">,</span><span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x_1$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x_2$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_moons</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">700</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.30</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">nb_weak</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1"># Adaboost sur arbres de décision</span>
<span class="n">adaDT_clf</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> 
                               <span class="n">n_estimators</span><span class="o">=</span><span class="n">nb_weak</span><span class="p">,</span><span class="n">algorithm</span><span class="o">=</span><span class="s2">&quot;SAMME.R&quot;</span><span class="p">,</span> 
                               <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="c1"># Adaboost sur SVM</span>
<span class="n">adaSVC_clf</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s2">&quot;rbf&quot;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span><span class="n">gamma</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">),</span> 
                                <span class="n">n_estimators</span><span class="o">=</span><span class="n">nb_weak</span><span class="p">,</span><span class="n">algorithm</span><span class="o">=</span><span class="s2">&quot;SAMME&quot;</span><span class="p">,</span> 
                                <span class="n">learning_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="c1"># Classifieur de Bayes naif gaussien</span>
<span class="n">adaGAU_clf</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span><span class="n">GaussianNB</span><span class="p">(),</span> <span class="n">n_estimators</span><span class="o">=</span><span class="n">nb_weak</span><span class="p">,</span>
    <span class="n">algorithm</span><span class="o">=</span><span class="s2">&quot;SAMME.R&quot;</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="k">for</span> <span class="n">clf</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">title</span> <span class="ow">in</span> <span class="p">((</span><span class="n">adaDT_clf</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="s1">&#39;Arbres de décision&#39;</span><span class="p">),</span> 
                    <span class="p">(</span><span class="n">adaSVC_clf</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="s1">&#39;SVM&#39;</span><span class="p">),</span> 
                    <span class="p">(</span><span class="n">adaGAU_clf</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="s1">&#39;Classifieur naïf gaussien&#39;</span><span class="p">)):</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">i</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">plot_decision_boundary</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/adaboost_1_0.png" src="_images/adaboost_1_0.png" />
</div>
</div>
</div>
<div class="section" id="interpretation-statistique">
<h2>Interprétation statistique<a class="headerlink" href="#interpretation-statistique" title="Lien permanent vers ce titre">#</a></h2>
<p>Il est possible d’interpréter Adaboost en termes statistiques, pour justifier en partie le bon comportement du boosting en classification. Pour ce faire, on définit les classifieurs faibles comme des fonctions paramétriques <span class="math notranslate nohighlight">\(h_{\theta_i}(\mathbf x) = h_i(\mathbf x,\theta)\)</span>, <span class="math notranslate nohighlight">\(\theta\in\Theta\)</span>, et le résultat de la classification comme une combinaison linéaire de ces classifieurs faibles <span class="math notranslate nohighlight">\(h(\mathbf x)=\displaystyle\sum_{i=1}^n \alpha_i h_{\theta_i}(\mathbf x)\)</span>.</p>
<p>Une approche pour déterminer les <span class="math notranslate nohighlight">\(\theta_i\)</span> et les <span class="math notranslate nohighlight">\(\alpha_i\)</span> est d’ajouter séquentiellement au problème d’apprentissage les classifieurs faibles, sans ajuster les paramètres et les coefficients de la solution courante (<a class="reference internal" href="#adaboost-stat-algorithm">Algorithm 8</a>).</p>
<div class="proof algorithm admonition" id="adaboost-stat-algorithm">
<p class="admonition-title"><span class="caption-number">Algorithm 8 </span></p>
<div class="algorithm-content section" id="proof-content">
<p><strong>Entrée</strong> : l’ensemble d’apprentissage <span class="math notranslate nohighlight">\(Z\)</span>, \mathbf x<span class="math notranslate nohighlight">\(, \)</span>M$</p>
<p><strong>Sortie</strong> : Un classifieur <span class="math notranslate nohighlight">\(h\)</span></p>
<p>Initialisation des poids <span class="math notranslate nohighlight">\(\mathbf w\)</span> : <span class="math notranslate nohighlight">\(\forall i\in\{1\cdots n\},(w_i=\frac{1}{n})\)</span></p>
<p>Pour <span class="math notranslate nohighlight">\(i=1\)</span> à <span class="math notranslate nohighlight">\(M\)</span></p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(h_0(\mathbf x) = 0\)</span></p></li>
</ol>
<p>Pour <span class="math notranslate nohighlight">\(i=1\)</span> à <span class="math notranslate nohighlight">\(M\)</span></p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>1. $(\alpha_i,\theta_i) = arg\displaystyle\min_{\alpha\in\mathbb{R}^+,\theta\in\Theta} \displaystyle\sum_{j=1}^n L(y_jh_{i-1}(\mathbf x_j)+\alpha h_{\theta_i}(\mathbf x_j))$
2. $h_i(\mathbf x) = h_{i-1}(\mathbf x)+\alpha_i h_{\theta_i}(\mathbf x)$
</pre></div>
</div>
<p><span class="math notranslate nohighlight">\(h(\mathbf x)=\displaystyle\sum_{i=1}^n \alpha_i h_{\theta_i}(\mathbf x)\)</span></p>
</div>
</div><p>Cet algorithme, lorsque l’on utilise la fonction de perte exponentielle <span class="math notranslate nohighlight">\(L(y,f(\mathbf x)) = e^{-yf(\mathbf x)}\)</span>, est équivalent à l’algorithme Adaboost. En effet, à chaque itération, la minimisation suivante est effectuée :</p>
<div class="math notranslate nohighlight">
\[\begin{split} 
(\alpha_i,\theta_i) &amp;=&amp; arg\min_{\alpha\in\mathbb{R}^+,\theta\in\Theta} \displaystyle\sum_{j=1}^nexp\left [-y_j(h_{i-1}(\mathbf x_j)+\alpha h_{\theta_i}(\mathbf x_j)) \right ]\\
&amp;=&amp; arg\min_{\alpha\in\mathbb{R}^+,\theta\in\Theta} \displaystyle\sum_{j=1}^nexp\left [-y_jh_{i-1}(\mathbf x_j)\right ]exp\left [-y_j\alpha h_{\theta_i}(\mathbf x_j) \right ]\\
&amp;=&amp; arg\min_{\alpha\in\mathbb{R}^+,\theta\in\Theta} \displaystyle\sum_{j=1}^n w_j^i exp\left[-y_j\alpha h_{\theta_i}(\mathbf x_j) \right]
\label{eqWeakMin}
\end{split}\]</div>
<p>où <span class="math notranslate nohighlight">\(w_j^i = exp\left [-y_j h_{i-1}(\mathbf x_j) \right]\)</span> n’affecte pas le problème d’optimisation. Pour tout <span class="math notranslate nohighlight">\(\alpha&gt;0\)</span>, la fonction objectif peut être réécrite</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\theta_i &amp;=&amp; arg\min_{\theta\in\Theta} \left [e^{-\alpha}\displaystyle\sum_{y_j=h_{\theta_i}(\mathbf x_j)} w_j^t +e^\alpha \displaystyle\sum_{y_j\neq h_{\theta_i}(\mathbf x_j)} w_j^i \right ]\\
&amp;=&amp; arg\min_{\theta\in\Theta} \left [(e^{-\alpha}+e^\alpha)\displaystyle\sum_{j=1}^nw_j^i \mathbb{I}_{\{y_j\neq h_{\theta_i}(\mathbf x_j)\}} +e^\alpha \displaystyle\sum_{j=1}^n w_j^i \right ]\\
&amp;=&amp;arg\min_{\theta\in\Theta} \displaystyle\sum_{j=1}^nw_j^i \mathbb{I}_{\{y_j\neq h_{\theta_i}(\mathbf x_j)\}} \\
\end{split}\]</div>
<p>Le classifieur faible minimisant (\ref{eqWeakMin}) minimisera donc également le taux d’erreur pondéré, que l’on réinjecte dans (\ref{eqWeakMin}) pour trouver</p>
<div class="math notranslate nohighlight">
\[\alpha_i=\frac{1}{2}log\frac{1-\epsilon_i}{\epsilon_i}\]</div>
<p>avec</p>
<div class="math notranslate nohighlight">
\[\epsilon_i= \displaystyle\sum_{j=1}^nw_j^i \mathbb{I}_{\{y_j\neq h_{\theta_i}(\mathbf x_j)\}}\]</div>
<p>Enfin, la mise à jour du modèle <span class="math notranslate nohighlight">\(h_i(\mathbf x) = h_{i-1}(\mathbf x)+\alpha_ih_{\theta_i}(\mathbf x)\)</span> est équivalente à la mise à jour des poids dans AdaBoost, puisque</p>
<div class="math notranslate nohighlight">
\[w_j^{i+1}=w_j^{i}e^{-y_jh_i(\mathbf x_j)}\]</div>
<p>En résumé, AdaBoost peut être interprété comme un algorithme minimisant la fonction de perte exponentielle par ajout itératif de classifieurs faibles.</p>
</div>
<div class="section" id="interpretation-comme-probleme-de-maximisation-d-une-marge">
<h2>Interprétation comme problème de maximisation d’une marge<a class="headerlink" href="#interpretation-comme-probleme-de-maximisation-d-une-marge" title="Lien permanent vers ce titre">#</a></h2>
<p>Il est également possible de relier AdaBoost à un problème de séparation à vaste marge.</p>
<p>Le principe est de construire un espace de coordonnées de dimension égale au nombre de classifieurs faibles, <span class="math notranslate nohighlight">\(M\)</span>, et pour <span class="math notranslate nohighlight">\(\mathbf u\in \mathbb{R}^M\)</span> de considérer les coordonnées <span class="math notranslate nohighlight">\(u_i\)</span> comme les sorties des classifieurs faibles <span class="math notranslate nohighlight">\(h_i\)</span>. On montre alors qu’AdaBoost est un algorithme itératif qui résout le problème d’optimisation suivant</p>
<div class="math notranslate nohighlight">
\[\hat{w} = arg \max_{\mathbf w\in\mathbb{R}^M}\min_{\mathbf{u_j}\in\mathbb{R}^M}\frac{y_j\mathbf w^T\mathbf{u_j}}{\|\mathbf w\|_{L_1}}\]</div>
<p>où <span class="math notranslate nohighlight">\(\mathbf{u_j}=(h_1(\mathbf x_j)\cdots h_M(\mathbf x_j))\)</span> et où le classifieur final est</p>
<div class="math notranslate nohighlight">
\[h(\mathbf x) = \displaystyle\sum_{i=1}^M\hat{w}_ih_i(\mathbf x)\]</div>
<p>ce qui correspond à un problème de maximisation de la plus petite des distance entre des points de classes différentes, soit un problème de maximisation de marge.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="baggingboosting.html" title="précédent page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">précédent</p>
            <p class="prev-next-title">Bootstraping et bagging</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="combinaison.html" title="suivant page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">suivant</p>
        <p class="prev-next-title">Méthodes de combinaison</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Vincent BARRA<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>