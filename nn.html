
<!DOCTYPE html>

<html lang="fr">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>SVM linéaire &#8212; Apprentissage automatique</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script src="_static/translations.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Recherche" href="search.html" />
    <link rel="prev" title="SVM linéaire" href="hmm.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="fr">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Apprentissage automatique</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Apprentissage automatique
                </a>
            </li>
        </ul>
        <p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="possible.html">
   Exemple introductif
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="approchestat.html">
   Modèle statistique de l’apprentissage
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="modelesup.html">
   Modèle du processus d’apprentissage supervisé
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Classification
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="bayes.html">
   Classifieur naïf de Bayes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="LDAQDA.html">
   Analyses discriminantes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="knn.html">
   K plus proches voisins
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="arbres_decision.html">
   Arbres de décision
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Méthodes à noyau
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="svmGeom.html">
   SVM linéaire
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kernelTrick.html">
   Astuce du noyau
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Méthodes d'ensemble
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="baggingboosting.html">
   Bootstraping et bagging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="adaboost.html">
   Boosting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="combinaison.html">
   Méthodes de combinaison
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="randomforest.html">
   Forêts aléatoires
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gradientboosting.html">
   Gradient Boosting
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Manifold learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="mds.html">
   Positionnement multidimensionnel
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="isomap.html">
   ISOMAP
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lle.html">
   Local Linear Embedding
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="manifold.html">
   Unification
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Chaînes de Markov cachées
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="hmm.html">
   SVM linéaire
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Réseaux de neurones
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   SVM linéaire
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/vbarra/anbook.git"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/vbarra/anbook.git/issues/new?title=Issue%20on%20page%20%2Fnn.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/nn.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   SVM linéaire
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hyperplan-separateur">
     Hyperplan séparateur
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#probleme-d-optimisation">
     Problème d’optimisation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#donnees-non-lineairement-separables">
     Données non linéairement séparables
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cas-multiclasses">
     Cas multiclasses
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#svm-non-lineaire">
   SVM non linéaire
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#utilisation-en-regression">
   Utilisation en régression
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>SVM linéaire</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   SVM linéaire
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hyperplan-separateur">
     Hyperplan séparateur
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#probleme-d-optimisation">
     Problème d’optimisation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#donnees-non-lineairement-separables">
     Données non linéairement séparables
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cas-multiclasses">
     Cas multiclasses
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#svm-non-lineaire">
   SVM non linéaire
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#utilisation-en-regression">
   Utilisation en régression
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <p>Les machines à vecteurs de support (SVM - Support vector Machines ou Séparateur à Vaste Marge) <span id="id1">[<a class="reference internal" href="svmGeom.html#id37">Vap95</a>]</span> ont été définies de nombreuses manières, et sont appliquées dans de nombreux domaines depuis quelques années.</p>
<p>Soit <span class="math notranslate nohighlight">\(Z = \{ \mathbf x_i , y_i\}, \ 1,\leq i\leq n, \mathbf x_i \in {R}^d, y_i \in \{-1,1\}\}\)</span> un ensemble d’apprentissage pour un problème de classification binaire.</p>
<div class="tex2jax_ignore mathjax_ignore section" id="svm-lineaire">
<h1>SVM linéaire<a class="headerlink" href="#svm-lineaire" title="Lien permanent vers ce titre">#</a></h1>
<div class="section" id="hyperplan-separateur">
<h2>Hyperplan séparateur<a class="headerlink" href="#hyperplan-separateur" title="Lien permanent vers ce titre">#</a></h2>
<p>Une approche traditionnelle pour introduire les SVM est de partir du concept d’hyperplan séparateur des exemples positifs et négatifs de l’ensemble d’apprentissage. On définit alors la marge comme la distance du plus proche exemple à cet hyperplan, et on espère intuitivement que plus grande sera cette marge, meilleure sera la capacité de généralisation de ce séparateur linéaire.\
Un hyperplan de <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> est défini par</p>
<div class="math notranslate nohighlight">
\[\mathbf w^T\mathbf x + b = 0\]</div>
<p><span class="math notranslate nohighlight">\(\mathbf w\)</span> étant le vecteur normal à l’hyperplan. La fonction</p>
<div class="math notranslate nohighlight">
\[\label{eq:f} 
f({\mathbf x}) = \textrm{sign}( {\mathbf w^T\mathbf x} + b )
\]</div>
<p>permet, si elle sépare les données d’apprentissage, de les classifier correctement.  Un tel hyperplan, représenté par (<span class="math notranslate nohighlight">\(\mathbf w,b)\)</span> peut également être exprimé par <span class="math notranslate nohighlight">\((\lambda \mathbf w,\lambda b), \lambda\in\mathbb{R}\)</span>. Il est donc nécessaire de définir l’hyperplan canonique comme étant celui éloigné des données d’une distance au moins égale à 1. En fait, on impose qu’un exemple au moins de chaque classe soit à distance égale à 1. On considère alors le couple <span class="math notranslate nohighlight">\((\mathbf w,b)\)</span> tel que :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf w^T\mathbf x_i + b \ge +1 \ \ \textrm{si} \ \ y_i = +1 \\
\mathbf w^T\mathbf x_i + b \le -1 \ \ \textrm{si} \ \ y_i = -1
\end{split}\]</div>
<p>ou de manière plus compacte</p>
<div class="math notranslate nohighlight">
\[\forall i\quad y_i (\mathbf  w^T\mathbf x_i + b) \ge 1\]</div>
<p>Puisque l’on cherche à avoir la marge la plus grande possible, il est alors intéressant de calculer la distance, au sens de la norme euclidienne, d’un point <span class="math notranslate nohighlight">\(\mathbf x_i\)</span> à cet hyperplan.  Cette distance est la longueur du vecteur reliant <span class="math notranslate nohighlight">\(\mathbf x_i\)</span> à sa projection sur l’hyperplan, et est donnée par :</p>
<div class="math notranslate nohighlight">
\[d\Big( (\mathbf w,b) \ , \ \mathbf x_i \Big)
= \frac{ y_i (\mathbf w^T\mathbf x_i + b) }{ \parallel \mathbf w \parallel } \ge \frac{1}{ \parallel \mathbf w \parallel }
\]</div>
<p>Intuitivement, on veut trouver l’hyperplan qui maximise la cette distance, pour les <span class="math notranslate nohighlight">\(\mathbf x_i\)</span> les plus proches. L’équation précédente permet d’affirmer que cela est réalisé en minimisant <span class="math notranslate nohighlight">\(\parallel \mathbf w \parallel\)</span>, sous les contraintes de bonne classification.</p>
</div>
<div class="section" id="probleme-d-optimisation">
<h2>Problème d’optimisation<a class="headerlink" href="#probleme-d-optimisation" title="Lien permanent vers ce titre">#</a></h2>
<p>Le problème s’écrit alors comme un problème de minimisation sous contraintes :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\min_\mathbf w\in\mathbb{R}^d\; \parallel \mathbf w \parallel^2\\
\textrm{sous }y_i(\mathbf  w^T\mathbf  x_i) \geq 1,\quad 1\leq i\leq n\\
\end{split}\]</div>
<p>En introduisant les multiplicateurs de Lagrange, le problème dual s’écrit :</p>
<div class="math notranslate nohighlight">
\[
min  \ W(\alpha) = -\displaystyle\sum_{i=1}^n{\alpha_i} +
\frac{1}{2} \displaystyle\sum_{i=1}^{n}\displaystyle \sum_{j=1}^ny_iy_j\alpha_i\alpha_j(\mathbf x_i ^T \mathbf x_j)  \]</div>
<p>sous</p>
<div class="math notranslate nohighlight">
\[\displaystyle \sum_{i=1}^n y_i\alpha_i = 0 \]</div>
<div class="math notranslate nohighlight">
\[(\forall1\leq i\leq n)\; 0 \le \alpha_i \le C\]</div>
<p>où <span class="math notranslate nohighlight">\(\mathbf {\alpha}\)</span> est le vecteur des <span class="math notranslate nohighlight">\(n\)</span> multiplicateurs de Lagrange à déterminer, et <span class="math notranslate nohighlight">\(C\)</span> est une constante. En définissant la matrice <span class="math notranslate nohighlight">\((H)_{ij} = y_iy_j(\mathbf x_i ^T \mathbf x_j)\)</span> et \mathbf{1} le vecteur de <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span> dont toutes les composantes sont égales à 1, le problème se réécrit comme un problème de programmation quadratique (QP) :</p>
<div class="math notranslate nohighlight">
\[\begin{split}
min \label{eq:qp1}  W(\alpha) = {-\alpha}^T \mathbf{1} + \frac{1}{2}\alpha^T H \alpha
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ &amp; \\
\textrm{sous } \label{eq:qp2}  \alpha^Ty = 0
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ &amp; \\
\label{eq:qp3}  {0} \le {\alpha} \le C\mathbf{1}
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ &amp;
\end{split}\]</div>
<p>pour lequel de nombreuses méthodes de résolution ont été développées.</p>
<p>En dérivant l’équation précédente, il est possible de montrer que l’hyperplan optimal (canonique) peut être écrit comme</p>
<div class="math notranslate nohighlight">
\[\label{eq:w} \mathbf w = \displaystyle\sum_{i=1}^n \alpha_i y_i \mathbf x_i
\]</div>
<p>et <span class="math notranslate nohighlight">\(\mathbf w\)</span> est donc juste une combinaison linéaire des exemples d’apprentissage.</p>
<p>On peut également montrer que</p>
<div class="math notranslate nohighlight">
\[(\forall 1\leq i\leq n)\quad \alpha_i(y_i(\mathbf w^T \mathbf x_i + b) - 1) = 0 
\]</div>
<p>ce qui exprime que lorsque <span class="math notranslate nohighlight">\(y_i(\mathbf w ^T \mathbf x_i + b) &gt; 1\)</span>, alors  <span class="math notranslate nohighlight">\(\alpha_i = 0\)</span> : seuls les points d’apprentissage les plus proches de l’hyperplan (tels que <span class="math notranslate nohighlight">\(\alpha_i &gt; 0\)</span>)  contribuent au calcul de ce dernier, et on les appelle les vecteurs de support.\</p>
<p>En supposant avoir résolu le problème QP, et donc en disposant du <span class="math notranslate nohighlight">\(\mathbf {\alpha}\)</span> qui permet de calculer le vecteur <span class="math notranslate nohighlight">\(w\)</span> optimal, il reste à déterminer le biais <span class="math notranslate nohighlight">\(b\)</span>. Pour cela, en prenant un exemple positif  <span class="math notranslate nohighlight">\(\mathbf x^+\)</span> et un exemple négatif  <span class="math notranslate nohighlight">\(\mathbf x^-\)</span> quelconques, pour lesquels</p>
<div class="math notranslate nohighlight">
\[\begin{split}(\mathbf w ^T \mathbf x^+ + b) = +1 \\
(\mathbf w ^T \mathbf x^- + b) = -1
\end{split}\]</div>
<p>on a</p>
<div class="math notranslate nohighlight">
\[b = - \frac{1}{2} ( \mathbf w ^T \mathbf x^+ + \mathbf w ^T \mathbf x^- )
\]</div>
<p>L’hyperplan ainsi défini a besoin de très peu de vecteurs de support (méthode éparse) (<a class="reference internal" href="svmGeom.html#svmlin-ref"><span class="std std-numref">Fig. 14</span></a>).</p>
<div class="figure align-default" id="svmlin-ref">
<img alt="_images/svmLin.png" src="_images/svmLin.png" />
<p class="caption"><span class="caption-number">Fig. 23 </span><span class="caption-text">Deux jeux de points à respectivement 50 et 500 points par classe, tirées selon les mêmes lois. Dans les deux cas, l’hyperplan est défini par un très faible nombre de vecteurs support (en vert)</span><a class="headerlink" href="#svmlin-ref" title="Lien permanent vers cette image">#</a></p>
</div>
<p>En Scikit-learn</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">LinearSVC</span>
<span class="n">X</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">svm_clf</span> <span class="o">=</span> <span class="n">LinearSVC</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-050</span><span class="p">)</span>
<span class="n">svm_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="section" id="donnees-non-lineairement-separables">
<h2>Données non linéairement séparables<a class="headerlink" href="#donnees-non-lineairement-separables" title="Lien permanent vers ce titre">#</a></h2>
<p>Il reste à préciser le rôle de la contrainte <span class="math notranslate nohighlight">\({0} \le {\alpha} \le C\mathbf{1}\)</span>.
Lorsque <span class="math notranslate nohighlight">\(C\rightarrow\infty\)</span>, l’hyperplan optimal est celui qui sépare totalement les données d’apprentissage (si tant est qu’il existe). Pour des valeurs de <span class="math notranslate nohighlight">\(C\)</span> « raisonnables », des erreurs de classification peuvent être acceptées par le classifieur (soft margin). Pour cela on introduit des variables d’écart <span class="math notranslate nohighlight">\(\xi_i\)</span> :</p>
<div class="math notranslate nohighlight">
\[\forall 1\leq i\leq n\quad y_i(\mathbf w ^T \mathbf x_i + b) &gt; 1-\xi_i\]</div>
<p>Les vecteurs de support vérifient l’égalité, et les anciennes contraintes peuvent être violées de deux manières :
- <span class="math notranslate nohighlight">\((\mathbf x_i,y_i)\)</span> est à distance inférieure à la marge, mais du bon côté de l’hyperplan
- <span class="math notranslate nohighlight">\((\mathbf x_i,y_i)\)</span> est du mauvais côté de l’hyperplan</p>
<p>L’objectif est alors de minimiser  la moyenne des erreurs de classification <span class="math notranslate nohighlight">\(\displaystyle\sum_{i=1}^n \mathbf{1}_{\xi_i&gt;0}\)</span>. Ce problème étant NP-complet (fonction non continue et dérivable), on lui préfère le problème suivant</p>
<div class="math notranslate nohighlight">
\[\begin{split}	Min \frac{1}{2}\mathbf w^T\mathbf w + C\displaystyle\sum_{i}^n \xi_i\\
	sous\;  y_i\left ( \mathbf w^T\mathbf x_i+b\right )= 1-\xi_i
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(C\)</span> représente alors un compromis entre la marge possible entre les exemples et le nombre d’erreurs admissibles.
Nous illustrons dans la suite deux situations influencées par <span class="math notranslate nohighlight">\(C\)</span> :</p>
<ul class="simple">
<li><p>La <a class="reference internal" href="svmGeom.html#soft1-ref"><span class="std std-numref">Fig. 15</span></a> présente une première illustration du rôle de <span class="math notranslate nohighlight">\(C\)</span> : dans le cas de données linéairement séparables, un <span class="math notranslate nohighlight">\(C\)</span> faible autorisera des  vecteurs à rentrer dans la marge (vert). Plus <span class="math notranslate nohighlight">\(C\)</span> devient grand, plus le nombre de vecteurs support diminue, pour ne laisser aucun vecteur à distance inférieure à la marge de l’hyperplan optimal</p></li>
</ul>
<div class="figure align-default" id="soft1-ref">
<img alt="_images/soft1.png" src="_images/soft1.png" />
<p class="caption"><span class="caption-number">Fig. 24 </span><span class="caption-text">Données linéairement séparables</span><a class="headerlink" href="#soft1-ref" title="Lien permanent vers cette image">#</a></p>
</div>
<ul class="simple">
<li><p>La <a class="reference internal" href="svmGeom.html#soft2-ref"><span class="std std-numref">Fig. 16</span></a> présente un ensemble de données non linéairement séparables. La valeur de <span class="math notranslate nohighlight">\(C\)</span> contrôle le nombre d’erreurs de classification dans le résultat final.</p></li>
</ul>
<div class="figure align-default" id="soft2-ref">
<img alt="_images/soft2.png" src="_images/soft2.png" />
<p class="caption"><span class="caption-number">Fig. 25 </span><span class="caption-text">Données non linéairement séparables</span><a class="headerlink" href="#soft2-ref" title="Lien permanent vers cette image">#</a></p>
</div>
<p>En Scikit-learn</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="n">X</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">svm_clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1E10</span><span class="p">)</span>
<span class="n">svm_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="section" id="cas-multiclasses">
<h2>Cas multiclasses<a class="headerlink" href="#cas-multiclasses" title="Lien permanent vers ce titre">#</a></h2>
<p>Deux stratégies sont possibles dans le cas multiclasse :</p>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="svm-non-lineaire">
<h1>SVM non linéaire<a class="headerlink" href="#svm-non-lineaire" title="Lien permanent vers ce titre">#</a></h1>
<p>Pour utiliser les SVM dans un contexte non linéaire, on profite de l”<a class="reference internal" href="kernelTrick.html"><span class="doc std std-doc">astuce du noyau</span></a> puisque le modèle s’écrit avec un produit scalaire canonique.</p>
<div class="margin sidebar">
<p class="sidebar-title">Exemple de regression</p>
<p><img alt="" src="_images/ridgeregression.png" /></p>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="utilisation-en-regression">
<h1>Utilisation en régression<a class="headerlink" href="#utilisation-en-regression" title="Lien permanent vers ce titre">#</a></h1>
<p>Il est également possible, en changeant les fonctions de perte,  d’utiliser les SVM  en régression non paramétrique (<span class="xref myst">SVR</span> : Support Vector Regression)
, i.e. approcher une fonction de <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> dans <span class="math notranslate nohighlight">\(\mathbb{R}^p\)</span> par les mêmes mécanismes d’optimisation.</p>
<p id="id2"><dl class="citation">
<dt class="label" id="id62"><span class="brackets">Cov65</span></dt>
<dd><p>Thomas M. Cover. Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition. <em>Electronic Computers, IEEE Transactions on</em>, EC-14(3):326–334, 1965. URL: <a class="reference external" href="http://hebb.mit.edu/courses/9.641/2002/readings/Cover65.pdf">http://hebb.mit.edu/courses/9.641/2002/readings/Cover65.pdf</a>.</p>
</dd>
<dt class="label" id="id52"><span class="brackets">VC71</span></dt>
<dd><p>V. N. Vapnik and A. Ya. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. <em>Theory of Probability and its Applications</em>, 16(2):264–280, 1971.</p>
</dd>
<dt class="label" id="id51"><span class="brackets">Vap91</span></dt>
<dd><p>Vladimir Vapnik. Principles of risk minimization for learning theory. In <em>NIPS</em>, 831–838. Morgan Kaufmann, 1991.</p>
</dd>
<dt class="label" id="id38"><span class="brackets"><a class="fn-backref" href="#id1">Vap95</a></span></dt>
<dd><p>Vladimir N. Vapnik. <em>The nature of statistical learning theory</em>. Springer-Verlag New York, Inc., New York, NY, USA, 1995. ISBN 0-387-94559-8.</p>
</dd>
</dl>
</p>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "vbarra/anbook.git",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="hmm.html" title="précédent page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">précédent</p>
            <p class="prev-next-title">SVM linéaire</p>
        </div>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Vincent BARRA<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>